* Axon model
# 简单Axon model guide 单node，怎么进去，怎么出来
input = Axon.input("data")
{init_fn, predict_fn} = Axon.build(input)
params = init_fn.(Nx.template({1, 8}, :f32), %{})
predict_fn.(params, Nx.iota({1, 8}))

# mnist例子
model = MnistAxon.build_model()
{init_fn, predict_fn} = Axon.build(model)
params = init_fn.(Nx.template({1, 784}, :f32), %{})
test_images = MnistAxon.load_test_images() |> Enum.take(1) |> hd()
predict_fn.(params, test_images) |> Nx.argmax(axis: 1)

trained_params = MnistAxon.train_model(model)
predict_fn.(trained_params, test_images) |> Nx.argmax(axis: 1)
Axon.predict(model, trained_params, test_images) |> Nx.argmax(axis: 1)

* Intro
# Build, train and use model
model = MnistAxon.build_model()
trained_params = MnistAxon.train_model(model)
Axon.predict(model, trained_params, test_images) |> Nx.argmax(axis: 1)

# Tensor: shape, rank, type
t = Nx.tensor([ [1,3,5], [2,4,6] ])
{2,3} = Nx.shape(t)
{:s, 64} = Nx.type(t)
2 = Nx.rank(t)

# slice
t = Nx.iota({8, 8})
Nx.slice(t, [3,4], [2,2])

# ReLU
Axon.Activations.relu(Nx.tensor([ -5, 0, 5 ]))

# braodcasting
t = Nx.broadcast(0, {2,3,2})
Nx.add(t, Nx.tensor([8,9]))
Nx.add(t, Nx.tensor([ [5,7],[8,9],[4,4] ]))

* Axon
alias ExLearning.Axon.XOR
model = XOR.build_model({nil, 1}, {nil, 1})
input = %{"num1" => Nx.tensor([ [0] ]), "num2" => Nx.tensor([ [1] ])}
Axon.Display.as_table(model, input) |> IO.puts()
XOR.train_model(model)

Axon.build(model)

* Axon model
** 模型本身带有softmax与外接softmax
alias ExLearning.Axon.Simple
model1 = Simple.build_simple()
model2 = Simple.build_simple_no_softmax()
model3 = Simple.extend_softmax(model2)

# 输出对比
{init_fn, _pred_fn} = Axon.build(model1)
params = init_fn.(Nx.template({1, 6}, :f32), %{})
# 直接带有softmax跟串联组合结果完全一样
Axon.predict(model1, params, Nx.tensor([ [1,2,3,3,2,1] ]))
Axon.predict(model3, params, Nx.tensor([ [1,2,3,3,2,1] ]))
# 不含softmax的model2输出, 再附加softmax就都一样了
output = Axon.predict(model2, params, Nx.tensor([ [1,2,3,3,2,1] ]))
Axon.Activations.softmax(output)

** 单独只含input
model0 = Simple.simple_input()
{init_fn, _} = Axon.build(model0)
%{} = params = init_fn.(Nx.template({1, 3}, :u8), %{})
out0 = Axon.predict(model0, %{}, Nx.tensor([1,1,1]))
Axon.Activations.softmax(out0)

# 简单模型只有输入加直接softmax
model02 = Simple.extend_softmax(model0)
Axon.predict(model02, %{}, Nx.tensor([1,1,1]))
Axon.predict(model02, %{}, Nx.tensor([1,1,-1]))
Axon.Activations.softmax(Nx.tensor([1,1,1]))
Axon.Activations.softmax(Nx.tensor([1,1,-1]))

** 单独求loss
# 模型直接得到结果
alias ExLearning.Axon.Simple
model = Simple.build_simple_no_softmax()
{init_fn, pred_fn} = Axon.build(model)
params = init_fn.(Nx.template({1, 6}, :f32), %{})
Axon.predict(model, params, Nx.tensor([ [0.1, 0.2, 0.3, 0.3, 0.2, 0.1] ]))

Axon.Losses.categorical_cross_entropy(Nx.tensor([0,0,1]), Nx.tensor([0.5, -0.1, 1.1]))
