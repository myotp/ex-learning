* Axon model
# 简单Axon model guide 单node，怎么进去，怎么出来
input = Axon.input("data")
{init_fn, predict_fn} = Axon.build(input)
params = init_fn.(Nx.template({1, 8}, :f32), %{})
predict_fn.(params, Nx.iota({1, 8}))

# mnist例子
model = MnistAxon.build_model()
{init_fn, predict_fn} = Axon.build(model)
params = init_fn.(Nx.template({1, 784}, :f32), %{})
test_images = MnistAxon.load_test_images() |> Enum.take(1) |> hd()
predict_fn.(params, test_images) |> Nx.argmax(axis: 1)

trained_params = MnistAxon.train_model(model)
predict_fn.(trained_params, test_images) |> Nx.argmax(axis: 1)
Axon.predict(model, trained_params, test_images) |> Nx.argmax(axis: 1)

* Intro
# Build, train and use model
model = MnistAxon.build_model()
trained_params = MnistAxon.train_model(model)
Axon.predict(model, trained_params, test_images) |> Nx.argmax(axis: 1)

# Tensor: shape, rank, type
t = Nx.tensor([ [1,3,5], [2,4,6] ])
{2,3} = Nx.shape(t)
{:s, 64} = Nx.type(t)
2 = Nx.rank(t)

# slice
t = Nx.iota({8, 8})
Nx.slice(t, [3,4], [2,2])

# ReLU
Axon.Activations.relu(Nx.tensor([ -5, 0, 5 ]))

# braodcasting
t = Nx.broadcast(0, {2,3,2})
Nx.add(t, Nx.tensor([8,9]))
Nx.add(t, Nx.tensor([ [5,7],[8,9],[4,4] ]))

* Axon
alias ExLearning.Axon.XOR
model = XOR.build_model({nil, 1}, {nil, 1})
input = %{"num1" => Nx.tensor([ [0] ]), "num2" => Nx.tensor([ [1] ])}
Axon.Display.as_table(model, input) |> IO.puts()
XOR.train_model(model)

Axon.build(model)

* Axon model
** 模型本身带有softmax与外接softmax
alias ExLearning.Axon.Simple
model1 = Simple.build_simple()
model2 = Simple.build_simple_no_softmax()
model3 = Simple.extend_softmax(model2)

# 输出对比
{init_fn, _pred_fn} = Axon.build(model1)
params = init_fn.(Nx.template({1, 6}, :f32), %{})
# 直接带有softmax跟串联组合结果完全一样
Axon.predict(model1, params, Nx.tensor([ [1,2,3,3,2,1] ]))
Axon.predict(model3, params, Nx.tensor([ [1,2,3,3,2,1] ]))
# 不含softmax的model2输出, 再附加softmax就都一样了
output = Axon.predict(model2, params, Nx.tensor([ [1,2,3,3,2,1] ]))
Axon.Activations.softmax(output)

** 单独只含input
model0 = Simple.simple_input()
{init_fn, _} = Axon.build(model0)
%{} = params = init_fn.(Nx.template({1, 3}, :u8), %{})
out0 = Axon.predict(model0, %{}, Nx.tensor([1,1,1]))
Axon.Activations.softmax(out0)

# 简单模型只有输入加直接softmax
model02 = Simple.extend_softmax(model0)
Axon.predict(model02, %{}, Nx.tensor([1,1,1]))
Axon.predict(model02, %{}, Nx.tensor([1,1,-1]))
Axon.Activations.softmax(Nx.tensor([1,1,1]))
Axon.Activations.softmax(Nx.tensor([1,1,-1]))

** 单独求loss
# 模型直接得到结果
alias ExLearning.Axon.Simple
model = Simple.build_simple_no_softmax()
{init_fn, pred_fn} = Axon.build(model)
params = init_fn.(Nx.template({1, 6}, :f32), %{})
Axon.predict(model, params, Nx.tensor([ [0.1, 0.2, 0.3, 0.3, 0.2, 0.1] ]))

Axon.Losses.categorical_cross_entropy(Nx.tensor([0,0,1]), Nx.tensor([0.5, -0.1, 1.1]))

** linear
model = Simple.simple_linear()
Axon.Display.as_table(model, Nx.template({5,1}, :u8)) |> IO.puts()
{init_fn, pred_fn} = Axon.build(model)
params = init_fn.(Nx.template({1, 1}, :u8), %{})

* Linear regression with Axon functions
alias ExLearning.Axon.NaiveLinearRegression
# y = 2*x + 3
# model
model = NaiveLinearRegression.build_model()
{init_fn, pred_fn} = Axon.build(model)

# init params
init_params = init_fn.(Nx.template({1, 1}, :u8), %{})

# pred_fn
pred_fn.(init_params, Nx.tensor([ [10], [20] ]))

# input predict
inputs = Nx.tensor([ [10],[11],[12],[15] ])
y_pred = Axon.predict(model, init_params, inputs)
y_true = [23, 25, 27, 33] |> Nx.tensor(type: :f32) |> Nx.reshape({4,1})

# MSE
mse_loss = Axon.Losses.mean_squared_error(y_true, y_pred)

# loss+grad
NaiveLinearRegression.loss_value_and_grad(pred_fn, init_params, inputs, y_true)
NaiveLinearRegression.my_loss_value_and_grad({1, 1}, inputs, y_true)

# objective
NaiveLinearRegression.objective(pred_fn, &Axon.Losses.mean_squared_error/2, init_params, inputs, y_true)

# SGD optimizer
{opt_init_fn, opt_update_fn} = Polaris.Optimizers.sgd(learning_rate: 0.001)
optimizer_state = opt_init_fn.(init_params)
current_params = init_params
{new_params, new_optimizer_state, loss} = NaiveLinearRegression.update(current_params, optimizer_state, inputs, y_true, opt_update_fn, pred_fn)
# 循环
{new_params, new_optimizer_state, loss} = NaiveLinearRegression.update(new_params, new_optimizer_state, inputs, y_true, opt_update_fn, pred_fn)

# All
model = ExLearning.Axon.NaiveLinearRegression.build_model()
inputs = Nx.tensor([ [10],[11],[12],[15] ])
y_true = Nx.tensor([ [23],[25],[27],[33] ])
ExLearning.Axon.NaiveLinearRegression.train_model(model, inputs, y_true, 100, 0.001)

# 最终训练结果
| iterations |     w |     b |
|------------+-------+-------|
|        100 | 2.152 | 1.130 |
|       1000 | 2.128 | 1.422 |
|      10000 | 2.024 | 2.711 |
|      50000 | 2.000 | 2.999 |
